# Описание веб-приложения для взаимодействия с локальной LLM

## 1. Видение приложения

**Краткое описание:**
Простое веб-приложение, предоставляющее минималистичный интерфейс для взаимодействия пользователя с локально запущенной большой языковой моделью (LLM) через Ollama.

**Цель:**
Основная цель приложения — обеспечить легкий доступ к функциональности локальной LLM для индивидуальных пользователей без необходимости использования командной строки или сложных инструментов, используя простой и интуитивно понятный веб-интерфейс.

**Задачи:**
* Реализовать базовый механизм доступа для различения пользователей (по логину).
* Создать интуитивно понятный интерфейс чата для отправки запросов и получения ответов.
* Обеспечить взаимодействие с локальным Ollama API для получения генераций от LLM.
* Обрабатывать и отображать ответы LLM в удобном формате.
* Предоставить функцию быстрого копирования ответов LLM.

## 2. Требования

**Функциональные требования:**
* **FR1:** Пользователь должен иметь возможность ввести логин email и пароль для доступа к чату.
* **FR2:** Приложение должно отображать интерфейс чата после успешного ввода логина email и пароля.
* **FR3:** Пользователь должен иметь возможность ввести текстовый запрос в поле ввода.
* **FR4:** Пользователь должен иметь возможность отправить запрос, нажав кнопку или клавишу Enter.
* **FR5:** Отправленный запрос пользователя должен отображаться в окне чата.
* **FR6:** Приложение должно отправлять запрос пользователя на бэкенд для обработки LLM.
* **FR7:** Бэкенд должен отправлять полученный запрос в локальный Ollama API.
* **FR8:** Приложение должно получать ответ от Ollama через бэкенд.
* **FR9:** Ответ от LLM должен отображаться в окне чата под запросом пользователя.
* **FR10:** Приложение должно отображать заголовок перед сообщением пользователя и ответом LLM (например, "User спросил:", "LLAMA2 ответила:").
* **FR11:** Приложение должно предоставлять кнопку "Копировать" рядом с каждым ответом LLM.
* **FR12:** Нажатие кнопки "Копировать" должно копировать текст соответствующего ответа LLM в буфер обмена пользователя.
* **FR13:** Приложение должно обрабатывать ошибки (например, недоступность Ollama сервера) и сообщать о них пользователю.
* **FR14:** Интерфейс чата должен автоматически прокручиваться к последнему сообщению.

**Технические требования:**
* **TR1:** Бэкенд приложения должен быть реализован на Python с использованием фреймворка Flask.
* **TR2:** Фронтенд должен быть реализован с использованием HTML,и JavaScript.
* **TR3:** Взаимодействие между фронтендом и бэкендом должно осуществляться по протоколу HTTP (в частности, POST запросы для отправки запросов LLM).
* **TR4:** Взаимодействие между бэкендом и Ollama должно осуществляться через HTTP API Ollama.
* **TR5:** Приложение должно быть настроено для работы с сервером Ollama.
* **TR6:** Приложение должно использовать определенную модель LLM, доступную в Ollama. Должна быть возможность легкой смены модели (через конфигурацию в коде бэкенда).
* **TR7:** Приложение должно обрабатывать возможные задержки или тайм-ауты при обращении к Ollama.
* **TR8:** Для функции копирования должен использоваться Web Clipboard API (`navigator.clipboard`).

## 3. Use Cases 

  Use cases представлены в файле use_cases.md
```plantuml