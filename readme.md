# Описание веб-приложения для взаимодействия с локальной LLM

## 1. Видение приложения

**Краткое описание:**
Простое веб-приложение, предоставляющее минималистичный интерфейс для взаимодействия пользователя с локально запущенной большой языковой моделью (LLM) через Ollama.

**Цель:**
Основная цель приложения — обеспечить легкий доступ к функциональности локальной LLM для индивидуальных пользователей без необходимости использования командной строки или сложных инструментов, используя простой и интуитивно понятный веб-интерфейс.

**Задачи:**
* Реализовать базовый механизм доступа для различения пользователей (по логину).
* Создать интуитивно понятный интерфейс чата для отправки запросов и получения ответов.
* Обеспечить взаимодействие с локальным Ollama API для получения генераций от LLM.
* Обрабатывать и отображать ответы LLM в удобном формате.
* Предоставить функцию быстрого копирования ответов LLM.

## 2. Требования

**Функциональные требования:**
* **FR1:** Пользователь должен иметь возможность ввести логин email и пароль для доступа к чату.
* **FR2:** Приложение должно отображать интерфейс чата после успешного ввода логина email и пароля.
* **FR3:** Пользователь должен иметь возможность ввести текстовый запрос в поле ввода.
* **FR4:** Пользователь должен иметь возможность отправить запрос, нажав кнопку или клавишу Enter.
* **FR5:** Отправленный запрос пользователя должен отображаться в окне чата.
* **FR6:** Приложение должно отправлять запрос пользователя на бэкенд для обработки LLM.
* **FR7:** Бэкенд должен отправлять полученный запрос в локальный Ollama API.
* **FR8:** Приложение должно получать ответ от Ollama через бэкенд.
* **FR9:** Ответ от LLM должен отображаться в окне чата под запросом пользователя.
* **FR10:** Приложение должно отображать заголовок перед сообщением пользователя и ответом LLM (например, "User спросил:", "LLAMA2 ответила:").
* **FR11:** Приложение должно предоставлять кнопку "Копировать" рядом с каждым ответом LLM.
* **FR12:** Нажатие кнопки "Копировать" должно копировать текст соответствующего ответа LLM в буфер обмена пользователя.
* **FR13:** Приложение должно обрабатывать ошибки (например, недоступность Ollama сервера) и сообщать о них пользователю.
* **FR14:** Интерфейс чата должен автоматически прокручиваться к последнему сообщению.

**Технические требования:**
* **TR1:** Бэкенд приложения должен быть реализован на Python с использованием фреймворка Flask.
* **TR2:** Фронтенд должен быть реализован с использованием HTML,и JavaScript.
* **TR3:** Взаимодействие между фронтендом и бэкендом должно осуществляться по протоколу HTTP (в частности, POST запросы для отправки запросов LLM).
* **TR4:** Взаимодействие между бэкендом и Ollama должно осуществляться через HTTP API Ollama.
* **TR5:** Приложение должно быть настроено для работы с сервером Ollama.
* **TR6:** Приложение должно использовать определенную модель LLM, доступную в Ollama. Должна быть возможность легкой смены модели (через конфигурацию в коде бэкенда).
* **TR7:** Приложение должно обрабатывать возможные задержки или тайм-ауты при обращении к Ollama.
* **TR8:** Для функции копирования должен использоваться Web Clipboard API (`navigator.clipboard`).

## 3. Use Cases 

### Use Case 1: Отправка запроса языковой модели

```plantuml
@startuml
Actor Пользователь as User
System "Веб-приложение Ollama Agent" as App

User -> App : Ввести логин email, пароль и войти
App -> User : Отобразить страницу чата

User -> App : Ввести текстовый запрос
User -> App : Нажать кнопку "Отправить" / Enter

activate App
App -> App : Отобразить запрос пользователя в чате
App -> App : Отправить запрос на Бэкенд

activate App as Backend
Backend -> Ollama : Отправить запрос LLM API
activate Ollama
Ollama --> Backend : Вернуть ответ LLM
deactivate Ollama

Backend --> App : Передать ответ LLM
deactivate Backend

App -> App : Отобразить ответ LLM в чате (с кнопкой Копировать)
App -> User : Ответ LLM отображен
deactivate App
@enduml

#### Описание шагов:
1. Пользователь вводит свой логин, email, и пароль и входит в приложение.
2. Приложение отображает страницу чата.
3. Пользователь вводит текстовый запрос в поле ввода.
4. Пользователь инициирует отправку запроса (кнопка или Enter).
5. Приложение отображает запрос пользователя в области чата.
6. Приложение отправляет запрос на бэкенд.
7. Бэкенд формирует запрос к Ollama API и отправляет его.
8. Ollama обрабатывает запрос и генерирует ответ.
9. Ollama возвращает ответ бэкенду.
10. Бэкенд передает ответ фронтенду приложения.
11. Приложение отображает полученный ответ LLM в области чата.

### Use Case 2: Копирование ответа языковой модели 

@startuml
Actor Пользователь as User
System "Веб-приложение Ollama Agent" as App

User -> App : (Предпосылка: Ответ LLM отображен в чате)
App -> User : Отобразить кнопку "Копировать" рядом с ответом

User -> App : Нажать кнопку "Копировать"

activate App
App -> App : Выполнить копирование текста ответа в буфер обмена (используя JS/Web API)
App -> App : (Опционально) Изменить текст кнопки на "Скопировано!"
deactivate App

App --> User : Текст ответа скопирован в буфер обмена

@enduml

#### Описание шагов:
1. (Предпосылка) Пользователь получил и видит ответ от LLM в окне чата.
2. Приложение отображает кнопку "Копировать" рядом с этим ответом.
3. Пользователь нажимает на кнопку "Копировать".
4. Приложение с помощью JavaScript копирует текст соответствующего ответа в буфер обмена пользователя.
5. Текст ответа LLM доступен в буфере обмена пользователя для вставки в другие приложения. 